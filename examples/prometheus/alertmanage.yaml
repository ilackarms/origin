apiVersion: v1
kind: Template
metadata:
  name: prometheus
  annotations:
    "openshift.io/display-name": Prometheus
    description: |
      A monitoring solution for an OpenShift cluster - collect and gather metrics from nodes, services, and the infrastructure.
    iconClass: icon-cogs
    tags: "monitoring,prometheus,time-series"
parameters:
- description: The namespace to instantiate prometheus under. Defaults to 'default'.
  name: NAMESPACE
  value: default
objects:
- apiVersion: v1
  kind: ServiceAccount
  metadata:
    name: prometheus
    namespace: "${NAMESPACE}"
- apiVersion: v1
  kind: ClusterRoleBinding
  metadata:
    name: prometheus-cluster-reader
  roleRef:
    name: cluster-reader
  subjects:
  - kind: ServiceAccount
    name: prometheus
    namespace: "${NAMESPACE}"
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      prometheus.io/scrape: "true"
    labels:
      name: prometheus
    name: prometheus
    namespace: "${NAMESPACE}"
  spec:
    ports:
    - name: prometheus
      port: 80
      protocol: TCP
      targetPort: 9090
    selector:
      app: prometheus
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      prometheus.io/scrape: "true"
    labels:
      name: alertmanager
    name: alertmanager
    namespace: "${NAMESPACE}"
  spec:
    ports:
    - name: alertmanager
      port: 9093
      protocol: TCP
      targetPort: 9093
    selector:
      app: alertmanager
- apiVersion: extensions/v1beta1
  kind: Deployment
  metadata:
    labels:
      app: prometheus
    name: prometheus
    namespace: "${NAMESPACE}"
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: prometheus
    template:
      metadata:
        labels:
          app: prometheus
        name: prometheus
      spec:
        serviceAccountName: prometheus
        containers:
        - args:
          - -alertmanager.url=http://alertmanager:9093/
          - -storage.local.retention=6h
          - -storage.local.memory-chunks=500000
          - -config.file=/etc/prometheus/prometheus.yml
          image: prom/prometheus
          imagePullPolicy: IfNotPresent
          name: prometheus
          ports:
          - containerPort: 8080
            name: web
          volumeMounts:
          - mountPath: /etc/prometheus
            name: config-volume
          - mountPath: /prometheus
            name: data-volume
        restartPolicy: Always
        volumes:
        - configMap:
            defaultMode: 420
            name: prometheus
          name: config-volume
        - emptyDir: {}
          name: data-volume
- apiVersion: extensions/v1beta1
  kind: Deployment
  metadata:
    labels:
      app: alertmanager
    name: alertmanager
    namespace: "${NAMESPACE}"
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: alertmanager
    template:
      metadata:
        labels:
          app: alertmanager
        name: alertmanager
      spec:
        serviceAccountName: prometheus
        containers:
        - args:
          - -config.file=/etc/alertmanager/alertmanager.yaml
          image: quay.io/prometheus/alertmanager
          imagePullPolicy: IfNotPresent
          name: alertmanager
          ports:
          - containerPort: 9093
            name: web
          volumeMounts:
          - mountPath: /etc/alertmanager
            name: config-volume
          - mountPath: /alertmanager
            name: data-volume
        restartPolicy: Always
        volumes:
        - configMap:
            defaultMode: 420
            name: alertmanager
          name: config-volume
        - emptyDir: {}
          name: data-volume
- apiVersion: v1
  kind: ConfigMap
  metadata:
    name: alertmanager
    namespace: "${NAMESPACE}"
  data:
    alertmanager.yaml: |
      global:
        # The auth token for Hipchat.
        hipchat_auth_token: E77DVnSNpdFmApVtAgaAUtLdbhFRWWdli3kmOcoE
        # Alternative host for Hipchat.
        hipchat_url: 'https://prometheus-openshift-demo.hipchat.com/'

      # The directory from which notification templates are read.
      templates:
      - '/etc/alertmanager/template/*.tmpl'

      # The root route on which each incoming alert enters.
      route:
        # The labels by which incoming alerts are grouped together. For example,
        # multiple alerts coming in for cluster=A and alertname=LatencyHigh would
        # be batched into a single group.
        group_by: ['alertname', 'cluster', 'service']

        # When a new group of alerts is created by an incoming alert, wait at
        # least 'group_wait' to send the initial notification.
        # This way ensures that you get multiple alerts for the same group that start
        # firing shortly after another are batched together on the first
        # notification.
        group_wait: 10s

        # When the first notification was sent, wait 'group_interval' to send a batch
        # of new alerts that started firing for that group.
        group_interval: 10s

        # If an alert has successfully been sent, wait 'repeat_interval' to
        # resend them.
        repeat_interval: 3h

        # A default receiver
        receiver: team-X-hipchat

      receivers:
      - name: 'team-X-hipchat'
        hipchat_configs:
        - auth_token: E77DVnSNpdFmApVtAgaAUtLdbhFRWWdli3kmOcoE
          room_id: 3784528
          message_format: html
          notify: true
- apiVersion: v1
  kind: ConfigMap
  metadata:
    name: prometheus
    namespace: "${NAMESPACE}"
  data:
    prometheus.rules: |
      ALERT SecretRequestsHigh
        IF sum(apiserver_request_count{resource="secrets"}) > 100
        FOR 1s
        ANNOTATIONS {
          summary = "Instance {{ $labels.instance }} down",
          description = "{{ $labels.instance }} of job {{ $labels.job }} has been down for more than 5 minutes.",
        }
    prometheus.yml: |
      rule_files:
        - 'prometheus.rules'

      scrape_configs:

      - job_name: 'kubernetes-apiserver'
        kubernetes_sd_configs:
        - role: endpoints
        scheme: https
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
        relabel_configs:
        - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]
          action: keep
          regex: default;kubernetes;https

      - job_name: 'kubernetes-nodes'
        scheme: https
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
        kubernetes_sd_configs:
        - role: node

      - job_name: 'kubernetes-service-endpoints'
        kubernetes_sd_configs:
        - role: endpoints

      - job_name: 'kubernetes-pods'
        kubernetes_sd_configs:
        - role: pod